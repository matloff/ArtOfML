\documentclass{beamer} 
\usepackage{graphicx}
\usepackage{url}
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{listings}

\lstset{language=R}

\mode<presentation>
{ \usetheme{Hannover} }

\title{New Book, ``The Art of Machine Learning''}
\subtitle{and Intro to the qeML Package}
\author{Norm Matloff \\
University of California, Davis \\
}

\date{
East Bay R Users Group \\
December 12, 2023 
}

\begin{document} 

% \lstset{language=R}
\lstset{language=R,basicstyle=\smaller,commentstyle=\rmfamily\smaller,
   showstringspaces=false,
   xleftmargin=4ex,literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1}

\begin{frame}
\titlepage

% URL for these slides (repeated on final slide):
% \url{http://heather.cs.ucdavis.edu/parmatpows/Slides.pdf}

\end{frame}

\begin{frame} 
\frametitle{Why Yet Another ML Book?}

\begin{itemize}

\item Almost all books are either 

   \begin{itemize}

   \item math-heavy or

   \item ``cookbooks,'' step-by-step ``recipes,'' or

   \item both

   \end{itemize} 

\item ML is an \textit{art}, not a science

   \begin{itemize}

   \item Note my previous NSP ``Art of '' books:

   \begin{itemize}

      \item \textit{The Art of R Programming}
   
      \item \textit{The Art of Debugging}

   \end{itemize} 

   \item ML is typically taught in a ``What function should I call,
   and with what arguments?'' mode

   \end{itemize} 

\item My goal is to enable the reader to \textit{use} ML in the real
world.

\item NO MATH IS USED (just slope of line), but INTUITION is centrally
important.
What do these methods REALLY do?

\end{itemize} 

\end{frame} 

\begin{frame} 
\frametitle{Chapter Outline}

\begin{itemize}

\item Prologue: Regression problems, illustrated with k-NN

\item Prologue: Classification problems, illustrated with k-NN

\item Bias, Variance, Overfitting

\item Dealing with Large Numbers of Features

\item Decision Trees

\item Tweaking the Tress

\item Finding a Good Set of Hyperparamters

\item Linear, generalized linear models

\item Shrinkage-based models

\item Support Vector Machines

\item Neural networks

\item Image classification

\item Time Series and Text

\end{itemize} 

\end{frame} 

\begin{frame} 
\frametitle{Recurring Sections: the Bias-Variance Tradeoff}

\begin{itemize}

\item Supremely important---18,400,000 results to my Google query.

\item Yet most books just devote one or two \textit{very vague}
sentences to it.

\item Sections 1.7, all of Chapter 3, 4.3.6, 6.1, 6.3.5, 9.3.2, 11.10, 13.4

\item Example: k-Nearest Neighbors, Section 1.7

\begin{itemize}

\item if k is small, not many neighbors, a small ``sample''---hence large
\textbf{variance} 

\item if k is large, some neighbors are quite distant,
hence a \textbf{bias}; e.g. Y = weight, X = height

\end{itemize} 

\item Advantages and disadvantages of parametric models, including
polynomial regression.

\end{itemize} 

\end{frame} 

\begin{frame} 
\frametitle{Recurring Sections: Pitfalls}

\begin{itemize}

\item Sections 1.13, 1.14, 1.15, 1.16, 2.2.1, 2.2.2, 2.2.5, 2.4, 2.7.5,
5.3.1, 11.8, Appendix D

\item Example: Random Forests, Setion 5.3.1: 

   \begin{itemize}
   
   \item NYC taxi data (n=10000 version) 
   
   \item potentially 29,315
   pickup and dropoff combinations! 
   
   \item we aim roughly for $p < \sqrt{n}$ (though note \textit{double
   descent} etc.)
   
   \item \textbf{partykit} package error message, ``too many levels'' 
   
   \item possibly consolidate or even use latitude-longitude embedding
   
   \end{itemize} 

\end{itemize} 

\end{frame} 

\begin{frame} 
\frametitle{Statistics vs.\ CS}

\begin{itemize} 

\item Old Breiman ``Two Cultures'' essay still applies.

\item Sampling variation vs.\ ``the data.''

\item E.g. grid search for hyperparameter tuning includes standard errors.

\item Statistics $\iff$ CS Translator, e.g.\ \textit{prediction}
$\iff$ \textit{inference}

\end{itemize} 

\end{frame}

\begin{frame}[fragile] 
\frametitle{The qeML Package}

\begin{itemize}

\item On CRAN.

\item Independent of the book. 

\item ``Quick and Easy'' ML

\item Uniform, {\Large SIMPLE} user interface.  

\begin{lstlisting}
z <- qeRF(svcensus,'wageinc')
\end{lstlisting} 

One simple call, that's all!  No clumsy setup needed.

\item Various default options.

\item ``Easy for learners, powerful for advanced users''

\item Excellent for teaching:

\begin{itemize}

\item {\Large SIMPLE} user interface.

\item Many built-in datasets.

\item Includes a number of built-in ML tutorials vignettes, no
background needed.

\end{itemize} 

\item Various utlities, e.g.\ for factor manipulation.

\end{itemize} 

\end{frame} 

\begin{frame} 
\frametitle{Example: Comparison of Various ML Methods}

\begin{itemize}

\item All \textbf{qeML} predictive functions do automatic cross-validation.

\item Test accuracy in the \textbf{\$testAcc} component of the returned object.

\item Also \textbf{\$baseAcc}, accuracy of prediction without X, for
comparison.

\end{itemize} 

\end{frame} 

\begin{frame} 
\frametitle{Example}

Predict wage income in 2000 Census dataset, from age, gender, education and
tech occupation.

\includegraphics[scale=0.35]{cmp4ml.png}

Horizontal axis is (indexed) k, min leaf size etc.

Winner is good ol' polynomial regression!

\end{frame} 

\end{document} 

